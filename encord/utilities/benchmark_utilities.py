import dataclasses

from shapely.geometry import Point, Polygon, box

from encord.orm.label_row import LabelRow


@dataclasses.dataclass
class LabelAnnotationMetrics:
    precision: float
    recall: float


# ---------------------------------------------------------
#                   Helper functions
# ---------------------------------------------------------
def __add_frame_consensus_score(
    base_frame, comparing_frames, feature_node_hash_set, classifications_answers_list, threshold, consensus_score
):
    """
    Update consensus score when comparing base_frame's objects with consensus objects obtained from comparing_frames.
    Two objects' instances are considered the same object if their Jaccard similarity coefficient is greater or equal
    than the threshold; otherwise, they represent distinct objects.
    """
    consensus_objects = __find_consensus_objects(comparing_frames, feature_node_hash_set, threshold)
    # Score each base_frame's object as true positive (TP) or false positive (FP) when compared to consensus objects
    for base_object in base_frame["objects"]:
        feature_hash = base_object["featureHash"]
        if feature_hash not in feature_node_hash_set:
            continue
        # Compare base_object with consensus objects of the same type to search for base_object's most similar object
        max_similarity = (-1, -1)  # (best_jaccard_similarity_coefficient, index_of_selected_consensus_object)
        consensus_objects.setdefault(feature_hash, dict())
        for comparing_index, comparing_object in consensus_objects[feature_hash].items():
            cur_similarity = (calculate_jaccard_similarity(base_object, comparing_object), comparing_index)
            max_similarity = max(max_similarity, cur_similarity)
        score = consensus_score.setdefault(feature_hash, {"TP": 0, "FP": 0, "FN": 0})
        if max_similarity[0] >= threshold:
            score["TP"] += 1
            consensus_objects[feature_hash].pop(max_similarity[1])
        else:
            score["FP"] += 1
    # Score each unmatched consensus' object as false negative (FN)
    for feature_hash, objects in consensus_objects.items():
        score = consensus_score.setdefault(feature_hash, {"TP": 0, "FP": 0, "FN": 0})
        score["FN"] += len(objects)

    consensus_classifications = __find_consensus_classifications(
        comparing_frames, feature_node_hash_set, classifications_answers_list
    )

    for classification in base_frame["classifications"]:
        feature_hash = classification["featureHash"]
        if feature_hash not in feature_node_hash_set:
            continue
        consensus_classifications.setdefault(feature_hash, dict())
        classification_hash = classification["classificationHash"]
        score = consensus_score.setdefault(feature_hash, {"TP": 0, "FP": 0, "FN": 0})
        # classifications_answers_list[0] is base_frame's classification_answers dict
        for class_answer in classifications_answers_list[0][classification_hash]["classifications"]:
            answers = class_answer["answers"]
            attribute_feature_hash = class_answer["featureHash"]
            if isinstance(answers, str):  # text type classification's answer
                if attribute_feature_hash not in consensus_classifications[feature_hash].keys():
                    score["FP"] += 1
                elif consensus_classifications[feature_hash][attribute_feature_hash] == answers:
                    score["TP"] += 1
                    consensus_classifications[feature_hash].pop(attribute_feature_hash)
                else:
                    score["FP"] += 1
            else:  # radio button or checklist classification's answer (list instance)
                for answer in answers:
                    answer_hash = answer["featureHash"]
                    if answer_hash in consensus_classifications[feature_hash].keys():
                        score["TP"] += 1
                        consensus_classifications[feature_hash].pop(answer_hash)
                    else:
                        score["FP"] += 1

    # Score each unmatched consensus' classification as false negative (FN)
    for feature_hash, attributes in consensus_classifications.items():
        score = consensus_score.setdefault(feature_hash, {"TP": 0, "FP": 0, "FN": 0})
        score["FN"] += len(attributes)


def __find_consensus_objects(comparing_frames, feature_node_hash_set, threshold):
    """
    Compare objects generated by different agents across the same frame and return those where the majority of
    agents agreed on. Two objects' instances are considered the same object if their Jaccard similarity coefficient is
    greater or equal than the threshold; otherwise, they represent distinct objects.
    """
    comparing_objects = [frame["objects"] for frame in comparing_frames]

    # Cluster objects by their corresponding feature node hash in the ontology while keeping a separation between agents
    feature_hash_to_object_dict = dict()
    for objects in comparing_objects:
        feature_hash_to_objects = dict()
        for obj in objects:
            feature_hash = obj["featureHash"]
            if feature_hash in feature_node_hash_set:
                # Use dictionary so item deletion is O(1) in the average case instead of O(n)
                object_dict = feature_hash_to_objects.setdefault(feature_hash, dict())
                object_dict[len(object_dict)] = obj
        for feature_hash, object_dict in feature_hash_to_objects.items():
            object_dicts = feature_hash_to_object_dict.setdefault(feature_hash, [])
            object_dicts.append(object_dict)

    # Choose consensus objects as the ones appearing in at least half of the agents' annotations.
    consensus_objects = dict()
    amount_agents = len(comparing_frames)
    for feature_hash, object_dicts in feature_hash_to_object_dict.items():
        for base_index, base_dict in enumerate(object_dicts):
            objects_used_in_base_dict = list()
            for idx1, obj1 in base_dict.items():
                # Compare obj1 with all other agents' same object annotations to search the most similar object to obj1
                similarities_found = []  # Store tuples following (which_dict_the_object_came_from, object_index) format
                for comparing_index, comparing_dict in enumerate(object_dicts):
                    if base_index == comparing_index:  # skip comparison between same agent's annotations
                        continue
                    max_similarity = (-1, -1)  # (best_jaccard_similarity_coefficient, index_in_comparing_dict)
                    for idx2, obj2 in comparing_dict.items():
                        cur_similarity = (calculate_jaccard_similarity(obj1, obj2), idx2)
                        max_similarity = max(max_similarity, cur_similarity)
                    if max_similarity[0] >= threshold:
                        similarities_found.append((comparing_index, max_similarity[1]))

                # Check if the majority of agents agreed on obj1 and if so then delete those objects similar to
                # obj1 in order to avoid choosing other representation of the same object in a later iteration
                if (len(similarities_found) + 1) * 2 > amount_agents:
                    objects_used_in_base_dict.append(idx1)
                    # Use dictionary so item deletion is O(1) in the average case instead of O(n)
                    object_dict = consensus_objects.setdefault(feature_hash, dict())
                    object_dict[len(object_dict)] = obj1
                    for dict_index, object_index in similarities_found:
                        object_dicts[dict_index].pop(object_index)

            # Delete current agent's agreed by majority objects to avoid deleting entries in the dict while iterating it
            for object_index in objects_used_in_base_dict:
                object_dicts[base_index].pop(object_index)
    return consensus_objects


def __find_consensus_classifications(comparing_frames, feature_node_hash_set, classifications_answers_list):
    comparing_classifications = [frame["classifications"] for frame in comparing_frames]
    feature_hash_to_attribute_answers = dict()
    attributes_answers_count = dict()
    for index, classifications in enumerate(comparing_classifications):
        for classification in classifications:
            feature_hash = classification["featureHash"]
            if feature_hash not in feature_node_hash_set:
                continue
            classification_hash = classification["classificationHash"]
            attribute_answers = feature_hash_to_attribute_answers.setdefault(feature_hash, set())
            # classifications_answers_list[0] is base_frame's classification_answers dict, so the +1
            for class_answer in classifications_answers_list[index + 1][classification_hash]["classifications"]:
                answers = class_answer["answers"]
                attribute_feature_hash = class_answer["featureHash"]
                if isinstance(answers, str):  # text type classification's answer
                    attribute_answers.add(attribute_feature_hash)
                    str_to_count = attributes_answers_count.setdefault(attribute_feature_hash, dict())
                    str_to_count[answers] = str_to_count.get(answers, 0) + 1
                else:  # radio button or checklist classification's answer (list instance)
                    for answer in answers:
                        answer_hash = answer["featureHash"]
                        attribute_answers.add(answer_hash)
                        attributes_answers_count[answer_hash] = attributes_answers_count.get(answer_hash, 0) + 1

    consensus_classifications = dict()
    amount_agents = len(comparing_frames)
    for feature_hash, attribute_answers in feature_hash_to_attribute_answers.items():
        consensus_classifications[feature_hash] = dict()
        for attribute in attribute_answers:
            attribute_count = attributes_answers_count[attribute]
            if isinstance(attribute_count, dict):
                for answer, occurrences in attribute_count.items():
                    if 2 * occurrences > amount_agents:
                        consensus_classifications[feature_hash][attribute] = answer
            else:
                if 2 * attribute_count > amount_agents:
                    consensus_classifications[feature_hash][attribute] = attribute
        if len(consensus_classifications[feature_hash]) == 0:
            consensus_classifications.pop(feature_hash)

    return consensus_classifications


def calculate_jaccard_similarity(obj1, obj2):
    """
    Calculate Jaccard similarity coefficient (Intersection over Union) between objects obj1 and obj2.
    """
    if obj1["shape"] == "point" or obj2["shape"] == "point":
        raise NotImplementedError("Point object's shape doesn't yet support similarity calculation")
    if obj1["shape"] == "skeleton" or obj2["shape"] == "skeleton":
        raise NotImplementedError("Skeleton object's shape doesn't yet support similarity calculation")
    p1 = transform_object_dict_to_polygon(obj1)
    p2 = transform_object_dict_to_polygon(obj2)
    intersection = p1.intersection(p2).area
    union = p1.area + p2.area - intersection
    return intersection / union


def transform_object_dict_to_polygon(obj):
    """
    Return the polygon represented in obj.
    """
    if obj["shape"] == "bounding_box":
        x = obj["boundingBox"]["x"]
        y = obj["boundingBox"]["y"]
        h = obj["boundingBox"]["h"]
        w = obj["boundingBox"]["w"]
        return box(x, y, x + w, y + h)
    elif obj["shape"] == "polygon":
        poly_dict = obj["polygon"]
        return Polygon([(poly_dict[str(i)]["x"], poly_dict[str(i)]["y"]) for i in range(len(poly_dict))])
    elif obj["shape"] == "point":
        x = obj["point"]["0"]["x"]
        y = obj["point"]["0"]["y"]
        return Point(x, y)
    elif obj["shape"] == "skeleton":
        raise NotImplementedError("Skeleton object's shape is not fully functional so no transformation is available")
    else:
        raise ValueError("{0} is not a supported object's shape".format(obj["shape"]))


def extract_frames_within_label_row(label_row: LabelRow):
    """
    Extract all the frames within a LabelRow.

    Args:
        label_row: The LabelRow where the frames are going to be extracted.

    Returns:
        A dictionary following the pattern:
        {"frame": {"objects":[...], "classifications":[...]},
         "frame": {"objects":[...], "classifications":[...]}, ...}

    Raises:
        ValueError:
            If the LabelRow has an invalid format.
    """
    frames = dict()
    if label_row.data_type == "img_group":
        for data_unit in label_row.data_units.values():
            frames[data_unit["data_sequence"]] = data_unit["labels"]
    elif label_row.data_type == "video":
        for data_unit in label_row.data_units.values():
            frames = data_unit["labels"]
            break
    else:
        raise ValueError("{0} is not a supported LabelRow's data type".format(label_row.data_type))
    return frames
